<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Moin Nabi</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Kuldeep Purohit</name>
                  
                </p><p align="">I am a Senior Research Scientist at <a href="https://www.sap.com/trends/machine-learning.html">SAP Machine Learning Research</a> in Berlin.
</br>
		  </br>
		      Before that, I was a postdoctoral research fellow in <a href="http://disi.unitn.it/~mhug/index.html">Deep Relational Learning</a> group at the University of Trento with Professor <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a> and a visiting researcher in the CS department at the <a href="https://www.cs.washington.edu/">University of Washington</a> working with <a href="http://homes.cs.washington.edu/~ali/">Ali Farhadi</a>.
I did my PhD at the <a href="http://www.iit.it/">Italian Institute of Technology</a> where I was advised by Professor <a href="http://profs.sci.univr.it/~swan/">Vittorio Murino</a> and also working closely with Professor <a href="http://www0.cs.ucl.ac.uk/staff/m.pontil/">Massimiliano Pontil</a> from University College London.
	    </br></br>
		  I am so delighted to start computer vision research with Professor <a href="https://explorecourses.stanford.edu/instructor/mehrdads">Mehrdad Shahshahani</a> at <a href="http://www.ipm.ac.ir/">IPM Vision Group</a>.
I did my masters in AI and my bachelors in software engineering in Iran.
	  </br>

<!--I had the opportunity to work under
I started Computer Vision with Mehrdad Shahshahani
Prior to my Ph.D., I spent one year as a research assistant at MI&V lab in <a href="http://www.test.com">Sharif University of Technology</a>. I was also fortunate enough to be advised by Professor <a href="http://www.test.com">Mehrdad Shahshahani</a> at the <a href="http://www.test.com">IPM Vision Group</a> from 2009 until 2011. I used to collaborate with IPPR lab at <a href="http://www.test.com">Amirkabir University of Technology</a> under supervision of Professor <a href="http://www.test.com">Mohammad Rahmati</a> as well.
I received my Master Degree on Artificial Intelligence from Tehran Polytechnic and my Bachelor Degree on Software Engineering from Shomal University at Amol (my home town). -->
                </p><p align="center">
<a href="mailto:moin.nabi@unitn.it">Email</a> &nbsp;/&nbsp;
<a href="./files/cv.pdf">CV</a> &nbsp;/&nbsp;
<!--<a href="https://scholar.google.it/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<a href="./files/Thesis_compressed.pdf">Thesis</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?hl=en&user=31seHAMAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp; 
<a href="https://it.linkedin.com/pub/moin-nabi/3b/492/aa5"> LinkedIn </a>
                </p>
              </td>
              <!--<td width="33%"><img src="./img/moin_pic_cool.jpg"></td>-->
				<td> <img src="./img/moin_pic_cool_2.jpg" style="width: 200;"></td></tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Research</heading>
                <!--<p> I work primarily on computer vision, but I am also interested in machine learning and pattern recognition. The central goal of my research is to use vast amounts of data to understand the underlying semantics and structure of visual contents. I am especially interested in learning and recognizing visual object categories and understanding human behaviors. I spent my Ph.D. working on learning mid-level representations for visual recognition (image and video understanding) and now, I am more focused on learning deep neural networks from noisy and incomplete multi-modal data.</p>-->
              <p>My research lies at the intersection of machine learning, computer vision, and natural language processing with an emphasis on learning Deep Neural Networks with minimal supervision and noisy/incomplete multi-modal data.
		      </br></br>
		      <span class="highlight"><strong>Internship Position: </strong> I like working with students. If you're a PhD student interested in a research internship working with me in Berlin, please send me an email with your CV and research interests.</span>
		      </p>
		    </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
		      
		      
		<p> <strong>[2017.09.19]</strong> Our <a href="https://arxiv.org/abs/1708.09644">paper</a> is awarded the <strong><a href="http://2017.ieeeicip.org/AwardFinalist.asp">Best Student Paper</a></strong> in ICIP 2017. Congrats Mahdyar!</p>
                <p> <strong>[2017.05.15]</strong> I will join <strong><a href="https://icn.sap.com/home.html">SAP Machine Learning Research</a></strong> in Berlin, as a <strong>Senior Research Scientist</strong> in Deep Learning.</p>
		<p> <strong>[2017.04.01]</strong> Two papers are accepted in <strong><a href="http://acl2017.org/">ACL 2017</a>.</strong> Congrats Azad and Ravi!</p>
                <p> <strong>[2016.12.01]</strong> I will present <strong><a href="https://arxiv.org/pdf/1611.06764.pdf">Plug-and-Play Binary Quantization Layer</a></strong> at Workshop on Efficient Deep Learning at NIPS 2016!</p>
		<p> <strong>[2016.11.11]</strong> The <strong><a href="https://github.com/hosseinm/med">Motion Emotion Dataset (MED)</a></strong> is online!</p>
                <p> <strong>[2016.09.25]</strong> Our work is finalist for the <strong><a href="http://2016.ieeeicip.org/Awards.asp">Best Paper Award</a></strong> in ICIP 2016.</p>

              </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/SelfPaced.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1605.07651">
	<papertitle>Self-Paced Deep Learning for Weakly Supervised Object Detection</papertitle></a><br>E. Sangineto*, <strong>M. Nabi</strong>*, D. Culibrk and N. Sebe<br>
                  <em>arXiv:1605.07651v2</em>, 2017 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1605.07651.pdf">PDF</a> /
		  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a>
                </p><p></p>
                <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
</br></br>
<small>*Authors contributed equally</small>

		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/CNNPlugPlay.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1610.00307">
	<papertitle>Plug-and-Play CNN for Crowd  Analysis: An Application in Abnormal Event Detection</papertitle></a><br>
			M. Ravanbakhsh, <strong>M. Nabi</strong>, Mousavi, E. Sangineto and N. Sebe<br>
                  <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a>
                </p><p></p>
                <p>We present a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical Flow. The proposed method is validated on challenging abnormality detection datasets and the showed the superiority of our method compared to the state-of-the-arts.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/WeblyPatch_2.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Webly-supervised Subcategoty-aware Discriminative Patch</papertitle></a><br><strong>M. Nabi</strong>, S. Divvala, A. Farhadi<br>
                  <em>Technical Report</em>*, 2014 &nbsp; <br>
                  <a href="./files/UW2014_TR.pdf">PDF</a> /
                  <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>
                </p><p></p>
                <p>We study discovering a set of discriminative patches in subcategories of an object category, then train them in a webly-supervised fashion.
</br></br>
<small>*This work was done while I was at UW.</small>
                </p><p></p>
                <p></p>
              </td>
            </tr>



<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/BagOfPoselet.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Human Action Recognition in Still Images using Bag of Latent Poselets</papertitle>*</a><br><strong>M. Nabi</strong>, M. Rahmati<br>
                  <em>9th European Conference on Visual Media Production (CVMP)</em>, 2012 &nbsp;<br>
                  <a href="./files/CVMP2012_abstract.pdf">PDF</a> /
                  <a href="./files/CVMP2012.bib">bibtex</a>
                </p><p></p>
                <p>We represent human body poses in a single images by extracting the Poselet activation vectors on it, and recognize human activities in still images using the proposed bag of latent Poselets.
</br></br>
<small>*This work is based on my MS thesis at AUT.</small>
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Miscellaneous</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 3 (via two paths)
                  </font>
                <br>
		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon!</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
                    
          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}
                    
          </script>
        </td>
      </tr>
    </tbody></table>
  

</body></html>

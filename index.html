<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Kuldeep Purohit</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Kuldeep Purohit</name>
                  
                </p><p align="">I am a PhD student at <a href="http://www.ee.iitm.ac.in/ipcvlab/">Image Processing and Computer Vision Lab</a>, IIT Madras. I received my Bachelors Degree in Electrical Engineering from . <a href="http://www.iitmandi.ac.in/">Indian Institute of Technology Mandi, Himachal Pradesh</a> My research lies at the intersection of Image Processing/Computer Vision and Deep Learning. My recent works have focused on restoration of images and videos suffering from blur, low-resolution, and haze and their utilization for scene segmentation and estimation of 3D geometry and motion.
</br>
		  </br>
	<!--	      Before that, I was a postdoctoral research fellow in <a href="http://disi.unitn.it/~mhug/index.html">Deep Relational Learning</a> group at the University of Trento with Professor <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a> and a visiting researcher in the CS department at the <a href="https://www.cs.washington.edu/">University of Washington</a> working with <a href="http://homes.cs.washington.edu/~ali/">Ali Farhadi</a>.
I did my PhD at the <a href="http://www.iit.it/">Italian Institute of Technology</a> where I was advised by Professor <a href="http://profs.sci.univr.it/~swan/">Vittorio Murino</a> and also working closely with Professor <a href="http://www0.cs.ucl.ac.uk/staff/m.pontil/">Massimiliano Pontil</a> from University College London.
	    </br></br>
		  I am so delighted to start computer vision research with Professor <a href="https://explorecourses.stanford.edu/instructor/mehrdads">Mehrdad Shahshahani</a> at <a href="http://www.ipm.ac.ir/">IPM Vision Group</a>.
I did my masters in AI and my bachelors in software engineering in Iran.
	  </br>
-->
<!--I had the opportunity to work under
I started Computer Vision with Mehrdad Shahshahani
Prior to my Ph.D., I spent one year as a research assistant at MI&V lab in <a href="http://www.test.com">Sharif University of Technology</a>. I was also fortunate enough to be advised by Professor <a href="http://www.test.com">Mehrdad Shahshahani</a> at the <a href="http://www.test.com">IPM Vision Group</a> from 2009 until 2011. I used to collaborate with IPPR lab at <a href="http://www.test.com">Amirkabir University of Technology</a> under supervision of Professor <a href="http://www.test.com">Mohammad Rahmati</a> as well.
I received my Master Degree on Artificial Intelligence from Tehran Polytechnic and my Bachelor Degree on Software Engineering from Shomal University at Amol (my home town). -->
                </p><p align="center">
<!--<a href="ee14s007@ee.iitm.ac.in">Email</a> &nbsp;/&nbsp;
<!--<a href="./files/cv.pdf">CV</a> &nbsp;/&nbsp; -->
<!--<a href="https://scholar.google.it/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<!--<a href="./files/Thesis_compressed.pdf">Thesis</a> &nbsp;/&nbsp; -->
<!--<a href="https://scholar.google.com/citations?hl=en&user=31seHAMAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp; -->
<a href="https://www.linkedin.com/in/kuldeeppurohit3/"> LinkedIn </a> /&nbsp;
<a href="./files/CV_KuldeepPurohit2.pdf"> CV </a>
                </p>
              </td>
              <!--<td width="33%"><img src="./img/moin_pic_cool.jpg"></td>-->
				<td> <img src="./img/25400684.jpeg" style="width: 200;"></td></tr> 
          </tbody></table>
 <!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Research</heading>
                <p> I work primarily on computer vision, but I am also interested in machine learning and pattern recognition. The central goal of my research is to use vast amounts of data to understand the underlying semantics and structure of visual contents. I am especially interested in learning and recognizing visual object categories and understanding human behaviors. I spent my Ph.D. working on learning mid-level representations for visual recognition (image and video understanding) and now, I am more focused on learning deep neural networks from noisy and incomplete multi-modal data.</p>-->
              <!--<p>My research lies at the intersection of machine learning, computer vision, and natural language processing with an emphasis on learning Deep Neural Networks with minimal supervision and noisy/incomplete multi-modal data.
		      </br></br>
		      <span class="highlight"><strong>Internship Position: </strong> I like working with students. If you're a PhD student interested in a research internship working with me in Berlin, please send me an email with your CV and research interests.</span>
		      </p>
		    </td>
            </tr>
          </tbody></table>
-->
<!--SECTION -->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
		      
		      
		<p> <strong>[2017.09.19]</strong> Our <a href="https://arxiv.org/abs/1708.09644">paper</a> is awarded the <strong><a href="http://2017.ieeeicip.org/AwardFinalist.asp">Best Student Paper</a></strong> in ICIP 2017. Congrats Mahdyar!</p>
                <p> <strong>[2017.05.15]</strong> I will join <strong><a href="https://icn.sap.com/home.html">SAP Machine Learning Research</a></strong> in Berlin, as a <strong>Senior Research Scientist</strong> in Deep Learning.</p>
		<p> <strong>[2017.04.01]</strong> Two papers are accepted in <strong><a href="http://acl2017.org/">ACL 2017</a>.</strong> Congrats Azad and Ravi!</p>
                <p> <strong>[2016.12.01]</strong> I will present <strong><a href="https://arxiv.org/pdf/1611.06764.pdf">Plug-and-Play Binary Quantization Layer</a></strong> at Workshop on Efficient Deep Learning at NIPS 2016!</p>
		<p> <strong>[2016.11.11]</strong> The <strong><a href="https://github.com/hosseinm/med">Motion Emotion Dataset (MED)</a></strong> is online!</p>
                <p> <strong>[2016.09.25]</strong> Our work is finalist for the <strong><a href="http://2016.ieeeicip.org/Awards.asp">Best Paper Award</a></strong> in ICIP 2016.</p>

              </td>
            </tr>
          </tbody></table>
-->



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
		      
		      
		<p> <strong>[2019.05]</strong> I have been awarded Google Travel Grant to present our <a href="https://arxiv.org/abs/1804.02913">paper</a> at <strong><a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.</p>
		<p> <strong>[2019.04]</strong> Our team has claimed 1st position in CVPR-NTIRE 2019 Image Colorization Challenge. We are also among the finalists in Video Deblurring, Video Superresolution, and Image Dehazing challenges of <a href="http://www.vision.ee.ethz.ch/ntire19/">NTIRE 2019</a>.</p>		      
		<p> <strong>[2018.12]</strong> Our <a href="https://arxiv.org/abs/1708.09644">paper</a> is selected for the <a href="https://cvit.iiit.ac.in/icvgip18/bestpaperaward.php">Best Paper Award (Runner Up)</a> in ICVGIP 2018.</p>		
		<p> <strong>[2018.08]</strong> Our team is among the finalists in all the 3 tracks of <a href="https://pirm2018.org/">ECCV-PIRM 2018</a> Image Super-resolution Challenge.</p>	
		<p> <strong>[2017.01]</strong> Started working as a Research Intern at <a href="https://www.kla-tencor.com/">KLA-Tencor</a>, Chennai.</p>	
		<p> <strong>[2016.03]</strong> Started working on a sponsored Research Project with <a href="https://www.kla-tencor.com/">NIOT</a>, Ministry of Earth Sciences, Govt. of India.</p>			      

              </td>
            </tr>
          </tbody></table>

	  
	  
	  
	  <!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>

	  
	  

	  
	  
	
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/movie_small.gif" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Bringing Alive Blurred Moments</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Anshul Shah, and A.N. Rajagopalan<br>
                 			                  <strong>Accepted for Oral Presentation at IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019), Long Beach, CA, USA, June 2019</strong> <br>
<a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv version</a> /
<a href="./files/CVPR2019_supplementary.pdf">Supplementary</a> /
<a href="https://github.com/anshulbshah/Blurred-Image-to-Video">Project Page</a>			
	 <!-- 	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>Designed a deep convolutional architecture to extract a sharp video from a motion blurred
image. The first stage involves unsupervised training of a novel spatiotemporal network
for motion extraction from short video sequences. The above network is utilized for guided
training of a CNN which extracts the same motion embedding from a single blurred image.
The above networks are finally linked with our efficient deblurring network to generate the
sharp video. Our framework delivers state-of-the-art accuracy in single image deblurring
and video extraction while being faster and more compact.
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/thumb_400.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Mixed-Dense Connection Networks for Image and Video Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>
			                  <strong>Elsevier Neurocomputing (Special Issue on Deep Learning for Image Super-Resolution) 2019</strong> <br>
                <a href="./files/Neurocomputing_accepted_compressed.pdf">Accepted version</a>
	 <!--  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>Proposed a deep architecture for image and video super-resolution, which is built using efficient convolutional units we refer to as mixed-dense connection blocks, whose design combines the strengths of both residual and dense connection strategies, while overcoming their limitations. We enable efficient super-resolution for higher scale-factors through our scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. We analyze the effects of loss configurations and demonstrate their utility in enhancing complementary image qualities. The proposed networks lead to state-of-the-art results on image and video super-resolution benchmarks.  
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>



<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./files/ICCV_thumbprint.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Spatially-Adaptive Residual Networks for Efficient Image and Video Deblurring</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>
                  <strong>Under Review</strong>* <br>
                  <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected 
			by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through
			a simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed.
			However, increasing the network capacity in this manner comes at the expense of an increase in model size and inference time,
			and ignoring the non-uniform nature of blur. We present a new architecture composed of spatially adaptive residual learning
			modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn 
			to modulate the filters. This capability is complemented by a self-attentive module which captures non-local relationships
			among the intermediate features and enhances the receptive field. We then incorporate a spatiotemporal recurrent module in 
			the design to also facilitate efficient video deblurring. Our networks can implicitly model the spatially-varying deblurring 
			process while dispensing with multi-scale processing and large filters entirely. Extensive qualitative and quantitative 
			comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed
			networks via a reduction in model-size and significant improvements in accuracy and speed, enabling almost real-time deblurring. 
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>




<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./files/Depth-map.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Planar Geometry and Latest Scene Recovery from a Single Motion Blurred Image</papertitle></a><br><strong>Kuldeep Purohit</strong>, Subeesh Vasu, M. Purnachandra Rao, and A.N. Rajagopalan<br>
                  <strong>Under Review</strong>* <br>
                  <a href="https://arxiv.org/abs/1904.03710">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>Existing works on motion deblurring either ignore the effects of depth-dependent blur or work with the assumption of a 
			multi-layered scene wherein each layer is modeled in the form of fronto-parallel plane. In this work, we consider
			the case of 3D scenes with piecewise planar structure i.e., a scene that can be modeled as a combination of multiple 
			planes with arbitrary orientations. We first propose an approach for estimation of normal of a planar scene from a 
			single motion blurred observation. We then develop an algorithm for automatic recovery of a number of planes, the 
			parameters corresponding to each plane, and camera motion from a single motion blurred image of a multiplanar 3D scene.
			Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by 
			adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal 
			that our proposed method achieves state-of-the-art results.  
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>



<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/UW_Dehazing.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Multi-level Weighted Enhancment for Underwater Image Dehazing</papertitle></a><br><strong>Kuldeep Purohit</strong>, Srimanta Mandal and A.N. Rajagopalan<br>
                  <strong>Accepted at the Journal of the Optical Society of America A (JOSA-A) </strong>* <br>
                  <a href="./files/JOSAA_selected_results.pdf">Representative Qualitative Results</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>Attenuation and scattering of light are responsible for haziness in images of underwater
scenes. We propose an approach to reduce this effect, based on the underlying principle is that enhancement at different
levels of detail can undo the degradation caused by underwater haze. The depth information is
captured implicitly while going through different levels of details due to depth-variant nature of
haze. Hence, we judiciously assign weights to different levels of image details and reveal that
their linear combination along with the coarsest information can successfully restore the image.
Results demonstrate the efficacy of our approach as compared to state-of-the-art underwater
dehazing methods. 
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./files/ICCV_thumbprint.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>EFFICIENT MOTION DEBLURRING WITH FEATURE TRANSFORMATION AND SPATIAL ATTENTION</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>
                  <strong>Accepted at the IEEE International Conference on Image Processing (ICIP) 2019</strong>* <br>
                  <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> A preliminary version of our work "Spatially-Adaptive Residual Networks for Efficient Image and Video Deblurring"</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/PD_plot.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Scale-Recurrent Multi-residual Dense Network for Image Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>
			                  <strong>PIRM Workshop and Challenge, Eurpean Conference on Computer Vision Workshops (ECCVW 2018), Munich, Germany, September 2018</strong> <br>

 <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.pdf">Paper</a> /
 <a href="./files/Poster_ECCV2018.pdf">Poster</a> 

          <!--       <a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv Pre-print</a>
	  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>A preliminary version of our Neurocomputing work, presented at the European Conference on Computer Vision (ECCV) - Perceptual Image Restoration and Manipulation (PIRM) Workshop 2018. Our team REC-SR was a <strong>finalist in all three regions of the Super Resolution Challenge</strong> (https://www.pirm2018.org/PIRM-SR.html).
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>




<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/blur_detection.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	   <!--              <p><a href="https://arxiv.org/abs/1610.00307"> -->
	<papertitle>Learning based Blur Detection and Segmentation</papertitle></a><br>
		 <strong>Kuldeep Purohit</strong>, Anshul B. Shah, and A.N. Rajagopalan<br>
                  <strong>IEEE International Conference on Image Processing (ICIP 2018), Athens, Greece, October 2018  </strong> <br>
                  <a href="https://ieeexplore.ieee.org/document/8451765">Paper Link</a> /
                  <a href="./files/ICIP2018_supplementary.pdf">Supplementary</a> /
                  <a href="./files/POSTER_ICIP2018.pdf">Poster</a> 
        <!--           <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a> -->
                </p><p></p>
                <p>We present a robust two-level architecture for blur-based segmentation of a single image. First network is a fully convolutional encoder-decoder for estimating a semantically meaningful blur map from the full-resolution blurred image. Second network is a CNN-based classifier for obtaining local (patch-level) blur-probabilities. Fusion of the two network outputs enables accurate blur-segmentation using Graph-cut optimization over the obtained probabilities. We also show its applications in blur magnification and matting.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/icvgip2019.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	   <!--              <p><a href="https://arxiv.org/abs/1610.00307"> -->
	<papertitle>Color Image Super Resolution in Real Noise</papertitle></a><br>
		 Srimanta Mandal, <strong>Kuldeep Purohit</strong>, and A.N. Rajagopalan<br>
                  <strong>Accepted for Oral Presentation at ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2018), IIIT Hyderabad, India, december 2018 </strong> <br>
                   <a href="./files/ICVGIP2018_Cam.pdf">Accepted Version</a> 
         <!--         <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a> -->
                </p><p></p>
                <p> Proposed an approach to super-resolve noisy color images by considering the color channels
jointly. Implicit low-rank structure of visual data is enforced via nuclear norm minimization
in association with color channel-dependent weights, which are added as a regularization
term to the cost function. Additionally, multi-scale details of the image are added to the
model through another regularization term that involves projection onto PCA basis, which
is constructed using similar patches extracted across different scales of the input image. <strong> Selected for the Best Paper Award (Runner Up) </strong>*: https://cvit.iiit.ac.in/icvgip18/bestpaperaward.php.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/ICVGIP.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Mosaicing Deep Underwater Imagery</papertitle></a><br><strong>Kuldeep Purohit</strong>,Subeesh Vasu, A.N. Rajagopalan, V Bala Naga Jyothi, and Ramesh Raju<br>
                  <strong>ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2016), IIT Guwahati, India, december 2016 </strong> &nbsp; <br>
                  <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /
		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">Supplementary</a> /
		<a href="./files/POSTER_ICVGIP2016.pdf">Poster</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p>This work deals with the problem of mosaicing deep underwater images (captured by Remotely Operated Vehicles), which suffer from haze, color-cast, and non-uniform illumination. We propose a framework that restores these images in accordance with a suitably derived degradation model. Furthermore, our scheme harnesses the scene-depth information present in the haze for non-rigid registration of the images before blending to construct a mosaic that is free from artifacts such as local blurring, ghosting, and visible seams.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/splicing.png" alt="PontTuset" width="170" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Splicing Localization in Motion Blurred 3D scenes</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>
                  <strong>IEEE International Conference on Image Processing (ICIP 2016), Phoenix, Arizona, September 2016</strong> &nbsp; <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/7533095">Paper</a> /				
                  <a href="./files/ICIP2016_supplementary.pdf">Supplementary</a> /
                  <a href="./files/POSTER_ICIP2016.pdf">Poster</a> 
      <!--            <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> This work proposes an efficient algorithm for depth based segmentation using spatially-distributed blur-kernels present in a single motion-blurred image of a 3D scene. The segmentation is then further utilized to estimate global camera motion from a single blurred image of a 3D scene. Finally, local blur profiles are compared with the global motion model to highlight inconsistencies and detect spliced regions. 
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/BagOfPoselet.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Human Action Recognition in Still Images using Bag of Latent Poselets</papertitle>*</a><br><strong>M. Nabi</strong>, M. Rahmati<br>
                  <em>9th European Conference on Visual Media Production (CVMP)</em>, 2012 &nbsp;<br>
                  <a href="./files/CVMP2012_abstract.pdf">PDF</a> /
                  <a href="./files/CVMP2012.bib">bibtex</a>
                </p><p></p>
                <p>We represent human body poses in a single images by extracting the Poselet activation vectors on it, and recognize human activities in still images using the proposed bag of latent Poselets.
</br></br>
<small>*This work is based on my MS thesis at AUT.</small>
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--SECTION -->




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Other Projects</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>

		    
		    

	              <td width="25%"><img src="./files/mstaa5c45f01_online.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Enhancement and Photo-metric Stereo for SEM images</papertitle></a><br><strong>Kuldeep Purohit</strong>, Arun M, Mahesh Mohan, and A.N. Rajagopalan<br>
                  <strong>Project with KLA-Tencor, India</strong> &nbsp; <br> 
                  2017 &nbsp; <br> 
              <!--    <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /
		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p> Addressed the blind reconstruction problem in scanning electron microscope (SEM) photometric stereo for complicated semiconductor patterns to be measured. 
			Developed a scheme using domain-specific priors on surface and sensor patterns in the optimization framework for robust estimation of the 3D surface structures.
			Also developed a user-centric detail enhancement scheme for improving visual quality of noisy SEM images.
			Proposed appraoch was validated through experiments on real data and was deployed commercially.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>		    
		    
		    
	              <td width="25%"><img src="./files/A_robot_competing_in_the_Intelligent_Ground_Vehicle_competition.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Autonomous Bot for the Annual Intelligent Ground Vehicle Competition (IGVC)</papertitle></a> 
                  <strong>Team Abhiyaan, Centre for Innovation (CFI), IIT Madras, India</strong> &nbsp; <br> 
                  2017 &nbsp; <br> 
                 <a href="http://www.igvc.org/design/2017/7.pdf">Report</a> 
	<!-- 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p> 
                <p> Our team designed a fully autonomous all-terrain ground vehicle. I specifically worked on the Computer Vision Module which involved algorithm development for 
	        real-time lane detection and obstacle segmentation task. The computer vision and path planning modules were integrated using ROS for autonomous navigation.
		The design was used in Vehicles that represented IIT Madras in the Intelligent Ground Vehicle Competition (IGVC) in 2017 and 2018.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

	              <td width="25%"><img src="./files/DRDO_thumbnail.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Multiple Target Detection and Tracking in Wide Area Surveillance</papertitle></a><br><strong>Kuldeep Purohit</strong> and Arshad Jamal<br> 
                  <strong>Project under Centre for Artificial Intelligence and Robotics, Defense Research and Development Organization, India</strong> &nbsp; <br> 
                  2012 &nbsp; <br> 
                 <a href="./files/DRDR_Report.pdf">Report</a> 
	<!-- 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p> 
                <p> In this project, the problem of object detection and tracking in the challenging domain of wide area surveillance 
			has been tackled. This problem poses several challenges: large camera motion, strong parallax, large number of moving 
			objects, and small number of pixels on target, single channel data and low frame-rate of video. The method implemented
			here overcomes these challenges when tested on UAV videos.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>
		    
<!--Next paper should be listed in bottom -->
<!--

	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--

	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>

-->

<!--                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 3 (via two paths)
                  </font>
                <br>
-->


 <!-- 		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
                    
          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}
                    
          </script>
        </td>
      </tr>
    </tbody></table>
 -->

	            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
<div style="text-align: left;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
              </td>
            </tr>
          </tbody></table>



	<!-- <script type="text/javascript" src="//ri.revolvermaps.com/0/0/1.js?i=834fq7qvtyr&amp;s=182&amp;m=0&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000"align="left" async="async"></script></div>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
</body></html>
